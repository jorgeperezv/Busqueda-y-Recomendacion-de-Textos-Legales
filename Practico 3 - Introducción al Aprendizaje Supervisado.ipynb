{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFsxMdtMauwY"
   },
   "source": [
    "### Universidad Nacional de Córdoba - Facultad de Matemática, Astronomía, Física y Computación\n",
    "\n",
    "#### Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones 2020\n",
    "\n",
    "Búsqueda y Recomendación de Textos Legales - Análisis y Curación de Datos\n",
    "\n",
    "Mentor: Claudio Sarate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6tndFHvOJWOn"
   },
   "source": [
    "Integrantes:\n",
    "* Clara Quintana\n",
    "* Ezequiel Juarez\n",
    "* David Veisaga\n",
    "* Jorge Pérez "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z6DUCN4Cauwc"
   },
   "source": [
    "### Práctico de Introducción al Aprendizaje Automático"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TjUT2z-eauwd"
   },
   "source": [
    "El objetivo de este práctico es desarrollar distintos modelos de clasificación para poder evaluar la performance y la exactitud de predicción de cada modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sE6B3U3Iauwe"
   },
   "source": [
    "### Requisitos iniciales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F07cHbOcauwg"
   },
   "source": [
    "El corpus debe estar ya depurado y debe poseer una columna con clases definidas previamente.\n",
    "\n",
    "Nota: es importante que la cantidad de las distintas clases sea medianamente balanceada para que el entrenamiento sea lo mas eficiente posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1144,
     "status": "ok",
     "timestamp": 1597359868160,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "-tfU92ikiqqI"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21347,
     "status": "ok",
     "timestamp": 1597359899237,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "jRcDtV9siuxE",
    "outputId": "35d1579e-0725-4e42-a7ce-d3512cd0fe6b"
   },
   "outputs": [],
   "source": [
    "# Se verfica entorno de ejecución\n",
    "in_colab = \"google.colab\" in sys.modules\n",
    "\n",
    "if in_colab:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "    BASE_DIR = \"/content/drive/My Drive/Diplo2020 Mentoria/\"\n",
    "else:\n",
    "    BASE_DIR = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1070,
     "status": "ok",
     "timestamp": 1597359902674,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "Ach9-FtsjKmv",
    "outputId": "93869852-ea4d-40e2-c532-59b0e927a4f6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TEXTO</th>\n",
       "      <th>DOCUMENTO</th>\n",
       "      <th>TIPO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sala electoral comp.originaria tribunal superi...</td>\n",
       "      <td>../Datos/Electoral//A 001-2018 COOPI C MUN CAR...</td>\n",
       "      <td>AUTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sala electoral comp.originaria tribunal superi...</td>\n",
       "      <td>../Datos/Electoral//A 002-2018 Denuncia R R RA...</td>\n",
       "      <td>AUTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sala electoral comp.originaria tribunal superi...</td>\n",
       "      <td>../Datos/Electoral//A 003-2018 VEDIA FLORES Cr...</td>\n",
       "      <td>AUTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sala electoral comp.originaria tribunal superi...</td>\n",
       "      <td>../Datos/Electoral//A 004-2018 FERNANDEZ Favio...</td>\n",
       "      <td>AUTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>sala electoral comp.originaria tribunal superi...</td>\n",
       "      <td>../Datos/Electoral//A 005-2018 ATANOR ADI ref.pdf</td>\n",
       "      <td>AUTO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              TEXTO  \\\n",
       "0           0  sala electoral comp.originaria tribunal superi...   \n",
       "1           1  sala electoral comp.originaria tribunal superi...   \n",
       "2           2  sala electoral comp.originaria tribunal superi...   \n",
       "3           3  sala electoral comp.originaria tribunal superi...   \n",
       "4           4  sala electoral comp.originaria tribunal superi...   \n",
       "\n",
       "                                           DOCUMENTO  TIPO  \n",
       "0  ../Datos/Electoral//A 001-2018 COOPI C MUN CAR...  AUTO  \n",
       "1  ../Datos/Electoral//A 002-2018 Denuncia R R RA...  AUTO  \n",
       "2  ../Datos/Electoral//A 003-2018 VEDIA FLORES Cr...  AUTO  \n",
       "3  ../Datos/Electoral//A 004-2018 FERNANDEZ Favio...  AUTO  \n",
       "4  ../Datos/Electoral//A 005-2018 ATANOR ADI ref.pdf  AUTO  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = BASE_DIR + \"corpus3.csv\"\n",
    "dataset = pandas.read_csv(train_data)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 714,
     "status": "ok",
     "timestamp": 1597359905013,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "ZxmTeqLH4Bw2",
    "outputId": "9fbf5682-e3d3-4d44-9c4a-dce3b07a16c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AUTO         125\n",
       "SENTENCIA     25\n",
       "Name: TIPO, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['TIPO'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKPdix9Xauwh"
   },
   "source": [
    "### Enunciado del práctico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "likRo76Yauwj"
   },
   "source": [
    " ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i1u9Iwv3auwk"
   },
   "source": [
    "Transformar el texto en vectores numéricos utilizando scikit-learn. Los procesos de vectorización, clasificación y evaluación de performance pueden ser hechos paso a paso o mediante el uso de pipelines para mayor eficiencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8G0hZ1UHy7x4"
   },
   "source": [
    "Scikit-learn ofrece 3 modelos de vectorización:\n",
    "\n",
    "* *CountVectorizer*: Convert a collection of text documents to a matrix of token counts\n",
    "* *TfidfVectorizer*: Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "*  *HashingVectorizer*: Convert a collection of text documents to a matrix of token occurrences\n",
    "\n",
    "Comparamos los 3 modelos usando el primer documento del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m-gUnniLz4DR"
   },
   "outputs": [],
   "source": [
    "# Texto del primer documento\n",
    "texto = dataset[0:1].TEXTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wp4Cs3EdyViK"
   },
   "source": [
    "*CountVectorizer*\n",
    "\n",
    "El recuento de palabras es un buen punto de partida, pero es muy básico. Un problema con los recuentos simples es que algunas palabras como “testamento” aparecerán muchas veces y sus recuentos grandes no serán muy significativos en los vectores codificados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gW5UDhi_nCxp"
   },
   "outputs": [],
   "source": [
    "vectorizer_1 = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 993,
     "status": "ok",
     "timestamp": 1597186415950,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "b-nNY2yonQC_",
    "outputId": "82ef2795-020a-4438-bd59-d68ca9c253d5"
   },
   "outputs": [],
   "source": [
    "# tokenizar y construir el vocabulario\n",
    "vectorizer_1.fit(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1128,
     "status": "ok",
     "timestamp": 1597186443930,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "AozY8ICVnQ9E",
    "outputId": "6870aba9-740e-4221-beba-f651bfdbeb7c"
   },
   "outputs": [],
   "source": [
    "# resumen\n",
    "print(vectorizer_1.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JmRFEx4ku1u6"
   },
   "outputs": [],
   "source": [
    "# codificador de documentos\n",
    "vector_1 = vectorizer_1.transform(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1710,
     "status": "ok",
     "timestamp": 1597186596972,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "fm9ttUKAvVIY",
    "outputId": "3876f297-c1c0-48f6-891d-c0cfaf598c60"
   },
   "outputs": [],
   "source": [
    "# resumir vector codificado\n",
    "print(vector_1.shape)\n",
    "print(type(vector_1))\n",
    "print(vector_1.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B7BdpWAxyvaD"
   },
   "source": [
    "*TfidfVectorizer*\n",
    "\n",
    "TF-IDF es un acrónimo que significa Frecuencia de Término – Frecuencia Inversa de Documento que son los componentes de las puntuaciones resultantes asignadas a cada palabra.\n",
    "\n",
    "* **Término Frecuencia**: Esto resume la frecuencia con la que una palabra dada aparece dentro de un documento.\n",
    "* **Frecuencia inversa de documentos**: Esto reduce la escala de las palabras que aparecen mucho en los documentos.\n",
    "\n",
    "El peso que tiene cada palabra ($w_i,_j$) es directamente proporcional a las veces que aparece en el documento ($tf_i,_j$) e inversamente proporcional a las veces que aparece en todos los documentos ($df_i$).\n",
    "\n",
    "$$ w_i,_j = tf_i,_j * log \\frac{N}{df_i} $$\n",
    "\n",
    "$tf_i,_j$ Frecuencia de la palabra $i$ en el documento $j$.\n",
    "\n",
    "$df_i$ Número de documentos que contienen la palabra $i$.\n",
    "\n",
    "$N$ Número total de documentos.\n",
    "\n",
    "\n",
    "TF-IDF son puntuaciones de frecuencia de palabras que tratan de resaltar las palabras que son más interesantes, por ejemplo, frecuentes en un documento pero no en todos los documentos.\n",
    "\n",
    "Los recuentos y las frecuencias pueden ser muy útiles, pero una limitación de estos métodos es que el vocabulario puede llegar a ser muy amplio. Esto, a su vez, requerirá grandes vectores para codificar los documentos e impondrá grandes requisitos a la memoria y a los algoritmos de ralentización. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DoI9NSQFy2ME"
   },
   "outputs": [],
   "source": [
    "vectorizer_2 = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1239,
     "status": "ok",
     "timestamp": 1597187961749,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "GQ5Ur13N0ibb",
    "outputId": "cdb0c540-4bbd-4159-898f-b5d1b5b218ff"
   },
   "outputs": [],
   "source": [
    "# tokenizar y construir el vocabulario\n",
    "vectorizer_2.fit(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 587
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 699,
     "status": "ok",
     "timestamp": 1597187988617,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "j6vqBC_n0pCq",
    "outputId": "c0786461-8435-471b-f81c-3a7d3f5ed67f"
   },
   "outputs": [],
   "source": [
    "# resumir\n",
    "print(vectorizer_2.vocabulary_)\n",
    "print(vectorizer_2.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hreUwZj601R7"
   },
   "outputs": [],
   "source": [
    "# documento codificado\n",
    "vector_2 = vectorizer_2.transform(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1134,
     "status": "ok",
     "timestamp": 1597188113927,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "XhysczUe1Hns",
    "outputId": "ff0fe514-bd8e-4fd9-bc7a-3cb423b6cd87"
   },
   "outputs": [],
   "source": [
    "# resumir vector codificado\n",
    "print(vector_2.shape)\n",
    "print(vector_2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I2V544vPz-7n"
   },
   "source": [
    "*HashingVectorizer*\n",
    "\n",
    "Podemos usar un hash de palabras para convertirlas en números enteros. La ventaja de esto, es que permite no tener vocabulario y poder elegir un vector de longitud fija arbitraria. Una desventaja es que el hash es una función unidireccional, por lo que no hay forma de volver a convertir la codificación en una palabra.\n",
    "\n",
    "La clase HashingVectorizer implementa este enfoque que se puede utilizar para convertir palabras en hash de forma coherente y, a continuación, convertir en token y codificar documentos según sea necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M5V73PLX7vRg"
   },
   "outputs": [],
   "source": [
    "vectorizer_3 = HashingVectorizer(n_features=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ubu0NgR7sz7"
   },
   "outputs": [],
   "source": [
    "# documento codificado\n",
    "vector_3 = vectorizer_3.transform(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 686,
     "status": "ok",
     "timestamp": 1597190135828,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "Ulib75Eq7pP9",
    "outputId": "8e92b2f5-5592-49bf-8b30-24826a6a9e88"
   },
   "outputs": [],
   "source": [
    "# resumir vector codificado\n",
    "print(vector_3.shape)\n",
    "print(vector_3.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d9RucZ6r_d-F"
   },
   "source": [
    "Al ejecutar el ejemplo se codifica el documento de muestra como una matriz dispersa de 300 elementos. Los valores del documento codificado corresponden a recuentos de palabras normalizados por defecto en el rango de -1 a 1, pero se pueden hacer recuentos enteros simples cambiando la configuración por defecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4TE8TH-E_17O"
   },
   "source": [
    "De los 3 métodos posibles utilizaremos TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NUUyaTF8auwm"
   },
   "source": [
    "### Dividir los datos en entrenamiento y validación con un procentaje de 70% para entrenamiento y 30% para validación con shuffle, seleccionar las features X e Y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<150x19220 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 204569 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit_transform(dataset['TEXTO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 791,
     "status": "ok",
     "timestamp": 1597360224486,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "dEwH0zG2ji9G"
   },
   "outputs": [],
   "source": [
    "# División entre instancias vectorizadas y etiquetas\n",
    "X, y = vectorizer.transform(dataset[\"TEXTO\"]), dataset[\"TIPO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# división entre entrenamiento y evaluación\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos en 5 subgrupos\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4IVpfRDPauwm"
   },
   "source": [
    " ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gl4z_0dYauwo"
   },
   "source": [
    "### Clasificar utilizando los datos de entrenamiento mediante Logistic Regresion, Naive Bayes, Random Forest y SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculo_metricas(tn, fp, fn, tp):\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "\n",
    "    if (tp + fp + fn + tn) > 0:\n",
    "        accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    if (tp + fp) > 0:\n",
    "        precision = (tp) / (tp + fp) \n",
    "    if (tp + fn) > 0:\n",
    "        recall = (tp) / (tp + fn)\n",
    "    if (precision + recall) > 0:\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imprimir_metricas(titulo, y_train, y_train_pred, y_test, y_test_pred):\n",
    "    print(titulo)\n",
    "    print('-' * len(titulo))\n",
    "\n",
    "    print('\\nEntrenamiento')\n",
    "    tn, fp, fn, tp = confusion_matrix(y_train, y_train_pred).ravel()\n",
    "\n",
    "    print(classification_report(y_train, y_train_pred))\n",
    "    print(f'Matriz de Confusión \\n\\tTP:{tp:{3}} \\tFP:{fp:{3}} \\n\\tTN:{tn:{3}} \\tFN:{fn:{3}}')\n",
    "\n",
    "    print('\\nEvaluación')\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
    "\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print(f'Matriz de Confusión \\n\\tTP:{tp:{3}} \\tFP:{fp:{3}} \\n\\tTN:{tn:{3}} \\tFN:{fn:{3}}')    \n",
    "    \n",
    "    return tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defino una función para cada uno de los modelos a comparar. \n",
    "\n",
    "* Los parámetros de entrada son los datos de entrenaimiento y evaluación.\n",
    "\n",
    "* Los parámetros de salida son las métricas de los datos de evaluación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_LogisticRegretion(X_train, y_train, X_test, y_test):\n",
    "    model = LogisticRegression(solver='lbfgs')\n",
    "    model.fit(X_train, y_train)\n",
    "   \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)    \n",
    "\n",
    "    tn, fp, fn, tp = imprimir_metricas('Modelo Logistic Regretion', y_train, y_train_pred, y_test, y_test_pred)\n",
    "    accuracy, precision, recall, f1 = calculo_metricas(tn, fp, fn, tp)\n",
    "    \n",
    "    return tp, fp, tn, fn, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_NaiveBayes(X_train, y_train, X_test, y_test):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)    \n",
    "\n",
    "    tn, fp, fn, tp = imprimir_metricas('Modelo Naive Bayes', y_train, y_train_pred, y_test, y_test_pred)\n",
    "    accuracy, precision, recall, f1 = calculo_metricas(tn, fp, fn, tp)\n",
    "        \n",
    "    return tp, fp, tn, fn, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_RandomForest(X_train, y_train, X_test, y_test):\n",
    "    model = ensemble.RandomForestClassifier(random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)    \n",
    "\n",
    "    tn, fp, fn, tp = imprimir_metricas('Modelo Random Forest', y_train, y_train_pred, y_test, y_test_pred)\n",
    "    accuracy, precision, recall, f1 = calculo_metricas(tn, fp, fn, tp)\n",
    "\n",
    "    return tp, fp, tn, fn, accuracy, precision, recall, f1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_SVM(X_train, y_train, X_test, y_test):\n",
    "    model = svm.SVC()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)    \n",
    "\n",
    "    tn, fp, fn, tp = imprimir_metricas('Modelo SVM', y_train, y_train_pred, y_test, y_test_pred)\n",
    "    accuracy, precision, recall, f1 = calculo_metricas(tn, fp, fn, tp)\n",
    "        \n",
    "    return tp, fp, tn, fn, accuracy, precision, recall, f1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iZYjYnaXauwp"
   },
   "source": [
    " ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yVwtrj_cauwq"
   },
   "source": [
    "### Realizar las predicciones para cada caso generando la matriz de confusión (plotear) y los reportes de performance con valores para precision, recall, f1-score y support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardaremos las metrícas en la siguiente variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo Logistic Regretion\n",
      "-------------------------\n",
      "\n",
      "Entrenamiento\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        AUTO       0.91      0.98      0.94        86\n",
      "   SENTENCIA       0.85      0.58      0.69        19\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       105\n",
      "   macro avg       0.88      0.78      0.82       105\n",
      "weighted avg       0.90      0.90      0.90       105\n",
      "\n",
      "Matriz de Confusión \n",
      "\tTP: 11 \tFP:  2 \n",
      "\tTN: 84 \tFN:  8\n",
      "\n",
      "Evaluación\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        AUTO       0.95      1.00      0.97        39\n",
      "   SENTENCIA       1.00      0.67      0.80         6\n",
      "\n",
      "   micro avg       0.96      0.96      0.96        45\n",
      "   macro avg       0.98      0.83      0.89        45\n",
      "weighted avg       0.96      0.96      0.95        45\n",
      "\n",
      "Matriz de Confusión \n",
      "\tTP:  4 \tFP:  0 \n",
      "\tTN: 39 \tFN:  2\n"
     ]
    }
   ],
   "source": [
    "# Entreno con Logistic Regretion\n",
    "tp, fp, tn, fn, accuracy, precision, recall, f1 = modelo_LogisticRegretion(X_train, y_train, X_test, y_test)\n",
    "model_list.append(['Logistic Regretion', tp, fp, tn, fn, accuracy, precision, recall, f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo Naive Bayes\n",
      "------------------\n",
      "\n",
      "Entrenamiento\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        AUTO       0.82      1.00      0.90        86\n",
      "   SENTENCIA       0.00      0.00      0.00        19\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       105\n",
      "   macro avg       0.41      0.50      0.45       105\n",
      "weighted avg       0.67      0.82      0.74       105\n",
      "\n",
      "Matriz de Confusión \n",
      "\tTP:  0 \tFP:  0 \n",
      "\tTN: 86 \tFN: 19\n",
      "\n",
      "Evaluación\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        AUTO       0.87      1.00      0.93        39\n",
      "   SENTENCIA       0.00      0.00      0.00         6\n",
      "\n",
      "   micro avg       0.87      0.87      0.87        45\n",
      "   macro avg       0.43      0.50      0.46        45\n",
      "weighted avg       0.75      0.87      0.80        45\n",
      "\n",
      "Matriz de Confusión \n",
      "\tTP:  0 \tFP:  0 \n",
      "\tTN: 39 \tFN:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jperezv\\Anaconda3\\envs\\mentoria\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Entreno con Naive Bayes\n",
    "tp, fp, tn, fn, accuracy, precision, recall, f1 = modelo_NaiveBayes(X_train, y_train, X_test, y_test)\n",
    "model_list.append(['Naive Bayes', tp, fp, tn, fn, accuracy, precision, recall, f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo Random Forest\n",
      "--------------------\n",
      "\n",
      "Entrenamiento\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        AUTO       0.99      1.00      0.99        86\n",
      "   SENTENCIA       1.00      0.95      0.97        19\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       105\n",
      "   macro avg       0.99      0.97      0.98       105\n",
      "weighted avg       0.99      0.99      0.99       105\n",
      "\n",
      "Matriz de Confusión \n",
      "\tTP: 18 \tFP:  0 \n",
      "\tTN: 86 \tFN:  1\n",
      "\n",
      "Evaluación\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        AUTO       0.95      1.00      0.97        39\n",
      "   SENTENCIA       1.00      0.67      0.80         6\n",
      "\n",
      "   micro avg       0.96      0.96      0.96        45\n",
      "   macro avg       0.98      0.83      0.89        45\n",
      "weighted avg       0.96      0.96      0.95        45\n",
      "\n",
      "Matriz de Confusión \n",
      "\tTP:  4 \tFP:  0 \n",
      "\tTN: 39 \tFN:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jperezv\\Anaconda3\\envs\\mentoria\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Entreno con Random Forest\n",
    "tp, fp, tn, fn, accuracy, precision, recall, f1 = modelo_RandomForest(X_train, y_train, X_test, y_test)\n",
    "model_list.append(['Random Forest', tp, fp, tn, fn, accuracy, precision, recall, f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo SVM\n",
      "----------\n",
      "\n",
      "Entrenamiento\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        AUTO       0.82      1.00      0.90        86\n",
      "   SENTENCIA       0.00      0.00      0.00        19\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       105\n",
      "   macro avg       0.41      0.50      0.45       105\n",
      "weighted avg       0.67      0.82      0.74       105\n",
      "\n",
      "Matriz de Confusión \n",
      "\tTP:  0 \tFP:  0 \n",
      "\tTN: 86 \tFN: 19\n",
      "\n",
      "Evaluación\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        AUTO       0.87      1.00      0.93        39\n",
      "   SENTENCIA       0.00      0.00      0.00         6\n",
      "\n",
      "   micro avg       0.87      0.87      0.87        45\n",
      "   macro avg       0.43      0.50      0.46        45\n",
      "weighted avg       0.75      0.87      0.80        45\n",
      "\n",
      "Matriz de Confusión \n",
      "\tTP:  0 \tFP:  0 \n",
      "\tTN: 39 \tFN:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jperezv\\Anaconda3\\envs\\mentoria\\lib\\site-packages\\sklearn\\daal4sklearn\\svm.py:332: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\jperezv\\Anaconda3\\envs\\mentoria\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Entreno con SVM\n",
    "tp, fp, tn, fn, accuracy, precision, recall, f1 = modelo_SVM(X_train, y_train, X_test, y_test)\n",
    "model_list.append(['SVM', tp, fp, tn, fn, accuracy, precision, recall, f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LKWHC4gTauwr"
   },
   "source": [
    " ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bORD4gTEauwt"
   },
   "source": [
    "### Determinar el modelo con mejor performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelo</th>\n",
       "      <th>tp</th>\n",
       "      <th>fp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fn</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regretion</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               modelo  tp  fp  tn  fn  accuracy  precision    recall   f1\n",
       "0  Logistic Regretion   4   0  39   2  0.955556        1.0  0.666667  0.8\n",
       "2       Random Forest   4   0  39   2  0.955556        1.0  0.666667  0.8\n",
       "1         Naive Bayes   0   0  39   6  0.866667        0.0  0.000000  0.0\n",
       "3                 SVM   0   0  39   6  0.866667        0.0  0.000000  0.0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pandas.DataFrame(model_list, columns=['modelo', 'tp', 'fp', 'tn', 'fn', 'accuracy', 'precision', 'recall', 'f1'])\n",
    "df.sort_values(by='accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2DhFvMC4auwu"
   },
   "source": [
    " ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BG0vrDxuauwv"
   },
   "source": [
    "### Probar con y sin shuffle en la partición de los datos y con distintos hiperparámetros para ver si los resultados cambian de acuerdo a si los datos se han mezclado al entrenar y validar o no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hN5s2dKtauww"
   },
   "source": [
    " ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-uk_AAf5MSL"
   },
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A9rpCJO4auwz"
   },
   "source": [
    " ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hXl82yFiauw0"
   },
   "source": [
    "### Entrega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hoAPkGzsauw1"
   },
   "source": [
    "Formato de entrega: Deberán utilizar esta notebook con los códigos con los que hicieron el análisis y los anaálisis y conclusiones despues de cada proceso. \n",
    "\n",
    "Fecha de entrega: 16/8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bGywguiPauw2"
   },
   "source": [
    " ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    AUTO\n",
       " Name: TIPO, dtype: object,\n",
       " 0    sala electoral comp.originaria tribunal superi...\n",
       " Name: TEXTO, dtype: object)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:1].TIPO, dataset[0:1].TEXTO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jperezv\\Anaconda3\\envs\\mentoria\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ensemble.RandomForestClassifier(random_state=0)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AUTO'], dtype=object)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x767 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 767 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(dataset[0:1].TEXTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 19220 and input n_features is 767 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-22974179cbdd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTEXTO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\mentoria\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \"\"\"\n\u001b[1;32m--> 545\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mentoria\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'estimators_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;31m# Check data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;31m# Assign chunk of trees to jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mentoria\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    362\u001b[0m                                  \"call `fit` before exploiting the model.\")\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mentoria\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    385\u001b[0m                              \u001b[1;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m                              \u001b[1;34m\"input n_features is %s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 19220 and input n_features is 767 "
     ]
    }
   ],
   "source": [
    "model.predict(vectorizer.transform(dataset[0:1].TEXTO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Z6DUCN4Cauwc",
    "oKPdix9Xauwh",
    "hXl82yFiauw0"
   ],
   "name": "Practico 3 - Introducción al Aprendizaje Supervisado.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:mentoria] *",
   "language": "python",
   "name": "conda-env-mentoria-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "output_auto_scroll": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
