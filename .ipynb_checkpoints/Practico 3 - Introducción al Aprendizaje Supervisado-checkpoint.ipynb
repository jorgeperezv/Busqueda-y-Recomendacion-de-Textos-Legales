{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFsxMdtMauwY"
   },
   "source": [
    "### Universidad Nacional de Córdoba - Facultad de Matemática, Astronomía, Física y Computación\n",
    "\n",
    "#### Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones 2020\n",
    "\n",
    "Búsqueda y Recomendación de Textos Legales - Análisis y Curación de Datos\n",
    "\n",
    "Mentor: Claudio Sarate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6tndFHvOJWOn"
   },
   "source": [
    "Integrantes:\n",
    "* Clara Quintana\n",
    "* Ezequiel Juarez\n",
    "* David Veisaga\n",
    "* Jorge Pérez "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z6DUCN4Cauwc"
   },
   "source": [
    "### Práctico de Introducción al Aprendizaje Automático"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TjUT2z-eauwd"
   },
   "source": [
    "El objetivo de este práctico es desarrollar distintos modelos de clasificación para poder evaluar la performance y la exactitud de predicción de cada modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sE6B3U3Iauwe"
   },
   "source": [
    "### Requisitos iniciales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F07cHbOcauwg"
   },
   "source": [
    "El corpus debe estar ya depurado y debe poseer una columna con clases definidas previamente.\n",
    "\n",
    "Nota: es importante que la cantidad de las distintas clases sea medianamente balanceada para que el entrenamiento sea lo mas eficiente posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1144,
     "status": "ok",
     "timestamp": 1597359868160,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "-tfU92ikiqqI"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.metrics import classification_report\n",
    "#import numpy as np\n",
    "#import seaborn as sns\n",
    "#import pandas as pd\n",
    "#import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21347,
     "status": "ok",
     "timestamp": 1597359899237,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "jRcDtV9siuxE",
    "outputId": "35d1579e-0725-4e42-a7ce-d3512cd0fe6b"
   },
   "outputs": [],
   "source": [
    "# Se verfica entorno de ejecución\n",
    "in_colab = \"google.colab\" in sys.modules\n",
    "\n",
    "if in_colab:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "    BASE_DIR = \"/content/drive/My Drive/Diplo2020 Mentoria/\"\n",
    "else:\n",
    "    BASE_DIR = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1070,
     "status": "ok",
     "timestamp": 1597359902674,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "Ach9-FtsjKmv",
    "outputId": "93869852-ea4d-40e2-c532-59b0e927a4f6"
   },
   "outputs": [],
   "source": [
    "train_data = BASE_DIR + \"corpus3.csv\"\n",
    "dataset = pandas.read_csv(train_data)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 714,
     "status": "ok",
     "timestamp": 1597359905013,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "ZxmTeqLH4Bw2",
    "outputId": "9fbf5682-e3d3-4d44-9c4a-dce3b07a16c4"
   },
   "outputs": [],
   "source": [
    "dataset['TIPO'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKPdix9Xauwh"
   },
   "source": [
    "### Enunciado del práctico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "likRo76Yauwj"
   },
   "source": [
    " ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i1u9Iwv3auwk"
   },
   "source": [
    "Transformar el texto en vectores numéricos utilizando scikit-learn. Los procesos de vectorización, clasificación y evaluación de performance pueden ser hechos paso a paso o mediante el uso de pipelines para mayor eficiencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8G0hZ1UHy7x4"
   },
   "source": [
    "Scikit-learn ofrece 3 modelos de vectorización:\n",
    "\n",
    "* *CountVectorizer*: Convert a collection of text documents to a matrix of token counts\n",
    "* *TfidfVectorizer*: Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "*  *HashingVectorizer*: Convert a collection of text documents to a matrix of token occurrences\n",
    "\n",
    "Comparamos los 3 modelos usando el primer documento del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m-gUnniLz4DR"
   },
   "outputs": [],
   "source": [
    "# Texto del primer documento\n",
    "texto = dataset[0:1].TEXTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wp4Cs3EdyViK"
   },
   "source": [
    "*CountVectorizer*\n",
    "\n",
    "El recuento de palabras es un buen punto de partida, pero es muy básico. Un problema con los recuentos simples es que algunas palabras como “testamento” aparecerán muchas veces y sus recuentos grandes no serán muy significativos en los vectores codificados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gW5UDhi_nCxp"
   },
   "outputs": [],
   "source": [
    "vectorizer_1 = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 993,
     "status": "ok",
     "timestamp": 1597186415950,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "b-nNY2yonQC_",
    "outputId": "82ef2795-020a-4438-bd59-d68ca9c253d5"
   },
   "outputs": [],
   "source": [
    "# tokenizar y construir el vocabulario\n",
    "vectorizer_1.fit(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1128,
     "status": "ok",
     "timestamp": 1597186443930,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "AozY8ICVnQ9E",
    "outputId": "6870aba9-740e-4221-beba-f651bfdbeb7c"
   },
   "outputs": [],
   "source": [
    "# resumen\n",
    "print(vectorizer_1.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JmRFEx4ku1u6"
   },
   "outputs": [],
   "source": [
    "# codificador de documentos\n",
    "vector_1 = vectorizer_1.transform(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1710,
     "status": "ok",
     "timestamp": 1597186596972,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "fm9ttUKAvVIY",
    "outputId": "3876f297-c1c0-48f6-891d-c0cfaf598c60"
   },
   "outputs": [],
   "source": [
    "# resumir vector codificado\n",
    "print(vector_1.shape)\n",
    "print(type(vector_1))\n",
    "print(vector_1.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B7BdpWAxyvaD"
   },
   "source": [
    "*TfidfVectorizer*\n",
    "\n",
    "TF-IDF es un acrónimo que significa Frecuencia de Término – Frecuencia Inversa de Documento que son los componentes de las puntuaciones resultantes asignadas a cada palabra.\n",
    "\n",
    "* **Término Frecuencia**: Esto resume la frecuencia con la que una palabra dada aparece dentro de un documento.\n",
    "* **Frecuencia inversa de documentos**: Esto reduce la escala de las palabras que aparecen mucho en los documentos.\n",
    "\n",
    "El peso que tiene cada palabra ($w_i,_j$) es directamente proporcional a las veces que aparece en el documento ($tf_i,_j$) e inversamente proporcional a las veces que aparece en todos los documentos ($df_i$).\n",
    "\n",
    "$$ w_i,_j = tf_i,_j * log \\frac{N}{df_i} $$\n",
    "\n",
    "$tf_i,_j$ Frecuencia de la palabra $i$ en el documento $j$.\n",
    "\n",
    "$df_i$ Número de documentos que contienen la palabra $i$.\n",
    "\n",
    "$N$ Número total de documentos.\n",
    "\n",
    "\n",
    "TF-IDF son puntuaciones de frecuencia de palabras que tratan de resaltar las palabras que son más interesantes, por ejemplo, frecuentes en un documento pero no en todos los documentos.\n",
    "\n",
    "Los recuentos y las frecuencias pueden ser muy útiles, pero una limitación de estos métodos es que el vocabulario puede llegar a ser muy amplio. Esto, a su vez, requerirá grandes vectores para codificar los documentos e impondrá grandes requisitos a la memoria y a los algoritmos de ralentización. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DoI9NSQFy2ME"
   },
   "outputs": [],
   "source": [
    "vectorizer_2 = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1239,
     "status": "ok",
     "timestamp": 1597187961749,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "GQ5Ur13N0ibb",
    "outputId": "cdb0c540-4bbd-4159-898f-b5d1b5b218ff"
   },
   "outputs": [],
   "source": [
    "# tokenizar y construir el vocabulario\n",
    "vectorizer_2.fit(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 587
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 699,
     "status": "ok",
     "timestamp": 1597187988617,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "j6vqBC_n0pCq",
    "outputId": "c0786461-8435-471b-f81c-3a7d3f5ed67f"
   },
   "outputs": [],
   "source": [
    "# resumir\n",
    "print(vectorizer_2.vocabulary_)\n",
    "print(vectorizer_2.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hreUwZj601R7"
   },
   "outputs": [],
   "source": [
    "# documento codificado\n",
    "vector_2 = vectorizer_2.transform(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1134,
     "status": "ok",
     "timestamp": 1597188113927,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "XhysczUe1Hns",
    "outputId": "ff0fe514-bd8e-4fd9-bc7a-3cb423b6cd87"
   },
   "outputs": [],
   "source": [
    "# resumir vector codificado\n",
    "print(vector_2.shape)\n",
    "print(vector_2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I2V544vPz-7n"
   },
   "source": [
    "*HashingVectorizer*\n",
    "\n",
    "Podemos usar un hash de palabras para convertirlas en números enteros. La ventaja de esto, es que permite no tener vocabulario y poder elegir un vector de longitud fija arbitraria. Una desventaja es que el hash es una función unidireccional, por lo que no hay forma de volver a convertir la codificación en una palabra.\n",
    "\n",
    "La clase HashingVectorizer implementa este enfoque que se puede utilizar para convertir palabras en hash de forma coherente y, a continuación, convertir en token y codificar documentos según sea necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M5V73PLX7vRg"
   },
   "outputs": [],
   "source": [
    "vectorizer_3 = HashingVectorizer(n_features=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ubu0NgR7sz7"
   },
   "outputs": [],
   "source": [
    "# documento codificado\n",
    "vector_3 = vectorizer_3.transform(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 686,
     "status": "ok",
     "timestamp": 1597190135828,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "Ulib75Eq7pP9",
    "outputId": "8e92b2f5-5592-49bf-8b30-24826a6a9e88"
   },
   "outputs": [],
   "source": [
    "# resumir vector codificado\n",
    "print(vector_3.shape)\n",
    "print(vector_3.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d9RucZ6r_d-F"
   },
   "source": [
    "Al ejecutar el ejemplo se codifica el documento de muestra como una matriz dispersa de 300 elementos. Los valores del documento codificado corresponden a recuentos de palabras normalizados por defecto en el rango de -1 a 1, pero se pueden hacer recuentos enteros simples cambiando la configuración por defecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4TE8TH-E_17O"
   },
   "source": [
    "De los 3 métodos posibles utilizaremos TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NUUyaTF8auwm"
   },
   "source": [
    "### Dividir los datos en entrenamiento y validación con un procentaje de 70% para entrenamiento y 30% para validación con shuffle, seleccionar las features X e Y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 791,
     "status": "ok",
     "timestamp": 1597360224486,
     "user": {
      "displayName": "Jorge Pérez Villella",
      "photoUrl": "",
      "userId": "05752208885034813709"
     },
     "user_tz": 180
    },
    "id": "dEwH0zG2ji9G"
   },
   "outputs": [],
   "source": [
    "# División entre instancias vectorizadas y etiquetas\n",
    "X, y = TfidfVectorizer().fit_transform(dataset[\"TEXTO\"]), dataset[\"TIPO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# división entre entrenamiento y evaluación\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos en 5 subgrupos\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4IVpfRDPauwm"
   },
   "source": [
    " ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gl4z_0dYauwo"
   },
   "source": [
    "### Clasificar utilizando los datos de entrenamiento mediante Logistic Regresion, Naive Bayes, Random Forest y SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculo_metricas(tn, fp, fn, tp):\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "\n",
    "    if (tp + fp + fn + tn) > 0:\n",
    "        accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    if (tp + fp) > 0:\n",
    "        precision = (tp) / (tp + fp) \n",
    "    if (tp + fn) > 0:\n",
    "        recall = (tp) / (tp + fn)\n",
    "    if (precision + recall) > 0:\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_LogisticRegretion(X_train, y_train, X_test, y_test):\n",
    "    model = LogisticRegression(solver='lbfgs')\n",
    "    model.fit(X_train, y_train)\n",
    "   \n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
    "\n",
    "    accuracy, precision, recall, f1 = calculo_metricas(tn, fp, fn, tp)\n",
    "    \n",
    "    return tp, fp, tn, fn, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_NaiveBayes(X_train, y_train, X_test, y_test):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
    "    \n",
    "    accuracy, precision, recall, f1 = calculo_metricas(tn, fp, fn, tp)\n",
    "        \n",
    "    return tp, fp, tn, fn, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_RamdomForest(X_train, y_train, X_test, y_test):\n",
    "    model = ensemble.RandomForestClassifier(random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
    "    \n",
    "    accuracy, precision, recall, f1 = calculo_metricas(tn, fp, fn, tp)\n",
    "        \n",
    "    return tp, fp, tn, fn, accuracy, precision, recall, f1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_SVM(X_train, y_train, X_test, y_test):\n",
    "    model = svm.SVC()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
    "    \n",
    "    accuracy, precision, recall, f1 = calculo_metricas(tn, fp, fn, tp)\n",
    "        \n",
    "    return tp, fp, tn, fn, accuracy, precision, recall, f1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = []\n",
    "grupo = 0\n",
    "for train_index, val_index in kf.split(X, y):\n",
    "    grupo += 1\n",
    "      \n",
    "    # Entreno con Logistic Regretion\n",
    "    tp, fp, tn, fn, accuracy, precision, recall, f1 = modelo_LogisticRegretion(X_train, y_train, X_val, y_val)\n",
    "    model_list.append([grupo, 'Logistic Regretion', tp, fp, tn, fn, accuracy, precision, recall, f1])\n",
    "\n",
    "    # Entreno con Naive Bayes\n",
    "    tp, fp, tn, fn, accuracy, precision, recall, f1 = modelo_NaiveBayes(X_train, y_train, X_val, y_val)\n",
    "    model_list.append([grupo, 'Naive Bayes', tp, fp, tn, fn, accuracy, precision, recall, f1])\n",
    "\n",
    "    # Entreno con Random Forest\n",
    "    tp, fp, tn, fn, accuracy, precision, recall, f1 = modelo_RandomForest(X_train, y_train, X_val, y_val)\n",
    "    model_list.append([grupo, 'Random Forest', tp, fp, tn, fn, accuracy, precision, recall, f1])\n",
    "\n",
    "    # Entreno con SVM\n",
    "    tp, fp, tn, fn, accuracy, precision, recall, f1 = modelo_SVM(X_train, y_train, X_val, y_val)\n",
    "    model_list.append([grupo, 'SVM', tp, fp, tn, fn, accuracy, precision, recall, f1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iZYjYnaXauwp"
   },
   "source": [
    " ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yVwtrj_cauwq"
   },
   "source": [
    "### Realizar las predicciones para cada caso generando la matriz de confusión (plotear) y los reportes de performance con valores para precision, recall, f1-score y support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LKWHC4gTauwr"
   },
   "source": [
    " ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bORD4gTEauwt"
   },
   "source": [
    "### Determinar el modelo con mejor performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.DataFrame(model_list, columns=['grupo', 'modelo', 'tp', 'fp', 'tn', 'fn', 'accuracy', 'precision', 'recall', 'f1'])\n",
    "df.sort_values(by='accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2DhFvMC4auwu"
   },
   "source": [
    " ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BG0vrDxuauwv"
   },
   "source": [
    "### Probar con y sin shuffle en la partición de los datos y con distintos hiperparámetros para ver si los resultados cambian de acuerdo a si los datos se han mezclado al entrenar y validar o no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hN5s2dKtauww"
   },
   "source": [
    " ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-uk_AAf5MSL"
   },
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A9rpCJO4auwz"
   },
   "source": [
    " ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hXl82yFiauw0"
   },
   "source": [
    "### Entrega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hoAPkGzsauw1"
   },
   "source": [
    "Formato de entrega: Deberán utilizar esta notebook con los códigos con los que hicieron el análisis y los anaálisis y conclusiones despues de cada proceso. \n",
    "\n",
    "Fecha de entrega: 16/8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bGywguiPauw2"
   },
   "source": [
    " ----------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Z6DUCN4Cauwc",
    "oKPdix9Xauwh",
    "hXl82yFiauw0"
   ],
   "name": "Practico 3 - Introducción al Aprendizaje Supervisado.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:mentoria] *",
   "language": "python",
   "name": "conda-env-mentoria-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "output_auto_scroll": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
